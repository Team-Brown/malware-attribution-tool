import os
import sys
import gensim
import pprint
from gensim import corpora
from collections import defaultdict
import collections

################################ BEGIN FUNCTION DEFINITIONS ################################

#d - opens all the files in the specified directory for reading
#p - opens all the files in parseInstructions directory for writing
#piPath - creates new files in parseInstructions directory; appends _INSTRUCTIONS.txt
def parseInstructions(d, p, piPath):
    lines = d.readlines() 

    for line in lines:
        if ("..." in line):
            continue

        if(len(line.split('\t')) >= 3):
            command = line.split('\t')[2]
            instruction = command.split(" ")[0]
        else:
            continue

        p.write(instruction + '\n')

    p.close()

    # same file you just closed but opened read-only
    instructions = open(piPath, "r")
    p = instructions.read()

    # texts is a list of all of the instructions
    texts = [word for word in p.lower().split("\n")]
    
    # remove empty items from texts
    for text in texts:
        if (text == " ") or (text == ""):
            texts.remove(text)

    line = ""
    
    for text in texts:
        line = line + " " + text
    
    return texts, line

def read_corpus(fname, tokens_only=False):
    with open(fname, encoding="iso-8859-1") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

# def languageAnalysis(corp):
#     for line in enumerate(corp):
#         tokens = gensim.utils.simple_preprocess(line)
#         yield tokens

################################# END FUNCTION DEFINITIONS #################################

# change directory to where the all samples are stored
if len(sys.argv) == 2:
    directory = sys.argv[1]
elif len(sys.argv) == 1:
    print("Please add the dir where samples are stored as an argument")
elif len(sys.argv) > 2:
    print("Invalid # of arguemnts - too many")

# create dir that disassembled binaries will go into
# outputDir_path = directory + "/objdumpOutput"

# if not os.path.isdir(outputDir_path):
#      os.mkdir(outputDir_path)

# print("\n*********************************************")
# print("Preparing to disassemble malware samples ...")
# print("*********************************************\n")

os.chdir(directory)

files = os.listdir('.')

for filename in files:
    if "corp" not in filename:
        objdumpOutput_path = "./objdumpOutput/" + filename + ".txt"
        print(objdumpOutput_path)
        print("Disassembling sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        cmd = "objdump -d " + filename + " > " + objdumpOutput_path
        os.system(cmd)

# for file in files:
#     objdumpOutput_path = "./objdumpOutput/" + file + ".txt"
#     print(objdumpOutput_path)
#     print("Disassembling sample " + str((files.index(file) + 1)) + " of " + str(len(files)) + "...")
#     cmd = "objdump -d " + file + " > " + objdumpOutput_path
#     os.system(cmd)

# print("\n! Completed disassmbling samples !\n")

#####################uncomment when ready to disassemble binaries###########################

# # REMINDER: still in the malware folder
instructions_path = directory + "parsedInstructions"

if not os.path.isdir("parsedInstructions"):
    os.mkdir("parsedInstructions")

print("\n*********************************************")
print("Preparing to parse out assembly instructions ...")
print("*********************************************\n")

# files = os.listdir("objdumpOutput")

# collection of all of the documents' instructions
test_corp_doc = open("test_corp_doc.txt", "w+")

for filename in files:
    if "corp" not in filename:
        o_path = "./objdumpOutput/" + filename
        i_path = "./parsedInstructions/" + filename[:-4] + "_INSTRUCTIONS.txt"

        o = open(o_path, 'r')
        i = open(i_path, 'w+')

        print("Parsing through sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        t, l = parseInstructions(o, i, i_path)

        test_corp_doc.write(l)
        test_corp_doc.write('\n')

        o.close()
        i.close()

# get_training_set(list_of_texts)
train_corp_doc = open("../training_samples/train_corp_doc.txt", "r")
train_corpus = list(read_corpus("../training_samples/train_corp_doc.txt"))

# print(train_corpus[:2])

test_corpus = list(read_corpus("test_corp_doc.txt", tokens_only=True))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
# ranks = []
# second_ranks = []
# for doc_id in range(len(train_corpus)):
#     inferred_vector = model.infer_vector(train_corpus[doc_id].words)
#     sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
#     rank = [docid for docid, sim in sims].index(doc_id)
#     ranks.append(rank)

    second_ranks.append(sims[1])

# counter = collections.Counter(ranks)
# print(counter)

# # print('Document ({}): «{}»\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
# print('Document ({}): «{}»\n'.format(doc_id, "..."))
# print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
# for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
#     # print(u'%s %s: «%s»\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))
#     print(u'%s %s: «%s»\n' % (label, sims[index], "..."))

# # Pick a random document from the corpus and infer a vector from the model
# import random
# doc_id = random.randint(0, len(train_corpus) - 1)

# # Compare and print the second-most-similar document
# # print('Train Document ({}): «{}»\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
# print('Train Document ({}): «{}»\n'.format(doc_id, "..."))
# sim_id = second_ranks[doc_id]
# # print('Similar Document {}: «{}»\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))
# print('Similar Document {}: «{}»\n'.format(sim_id, "..."))

# # Pick a random document from the test corpus and infer a vector from the model
# doc_id = random.randint(0, len(test_corpus) - 1)
# inferred_vector = model.infer_vector(test_corpus[doc_id])
# sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
# Compare and print the most/median/least similar documents from the train corpus
# print('Test Document ({}): «{}»\n'.format(doc_id, ' '.join(test_corpus[doc_id])))
print('Test Document ({}): «{}»\n'.format(doc_id, "..."))
print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
    # print(u'%s %s: «%s»\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))
    print(u'%s %s: «%s»\n' % (label, sims[index], "..."))

train_corp_doc.close()
test_corp_doc.close()