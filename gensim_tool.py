import os
import sys
import gensim
import pprint
from gensim import corpora
from collections import defaultdict
import collections

################################ BEGIN FUNCTION DEFINITIONS ################################

# TO DO: include comments to describe this function
def parseInstructions(o,i,i_path):
    lines = o.readlines()

    for line in lines:
        if ("..." in line):
            continue

        if(len(line.split('\t')) >= 3):
            command = line.split('\t')[2]
            instruction = command.split(" ")[0]
        else:
            continue

        i.write(instruction + '\n')

    i.close()

    # same file you just closed but opened read-only
    instructions = open(i_path, "r")
    i = instructions.read()

    # texts is a list of all of the instructions
    texts = [word for word in i.lower().split("\n")]

    # remove empty items from texts
    for text in texts:
        if (text == " ") or (text == ""):
            texts.remove(text)

    line = ""
    for text in texts:
        line = line + " " + text
    
    return texts, line

# def get_training_set(t):
#     doc = open("train_corp_doc.txt", "w+")
#     unique = []
#     for a in t:
#         for b in a:
#             if b not in unique:
#                 unique.append(b)
#                 x = b + " "
#                 doc.write(x)
#     doc.close()

def read_corpus(fname, tokens_only=False):
    with open(fname, encoding="iso-8859-1") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

def languageAnalysis(corp):
    for line in enumerate(corp):
        tokens = gensim.utils.simple_preprocess(line)
        yield tokens

################################# END FUNCTION DEFINITIONS #################################

# change directory to where the all samples are stored
if len(sys.argv) == 2:
    directory = sys.argv[1]
    print(directory)
elif len(sys.argv) == 1:
    print("Please add the dir where samples are stored as an argument")
elif len(sys.argv) > 2:
    print("Invalid # of arguemnts - too many")

# create dir that disassembled binaries will go into
outputDir_path = directory + "/objdumpOutput"
if not os.path.isdir(outputDir_path):
    os.mkdir(outputDir_path)

print("\n*********************************************")
print("Preparing to disassemble malware samples ...")
print("*********************************************\n")

os.chdir(directory)
files = os.listdir('.')
if "objdumpOutput" in files:
    files.remove('objdumpOutput')
if "parsedInstructions" in files:
    files.remove('parsedInstructions')

for filename in files:
    if "corp" not in filename:
        objdumpOutput_path = "./objdumpOutput/" + filename + ".txt"
        print(objdumpOutput_path)
        print("Disassembling sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        cmd = "objdump -d " + filename + " > " + objdumpOutput_path
        os.system(cmd)

print("\n! Completed disassmbling samples !\n")

# REMINDER: still in the malware folder
instructions_path = directory + "parsedInstructions"
if not os.path.isdir("parsedInstructions"):
    os.mkdir("parsedInstructions")

print("\n*********************************************")
print("Preparing to parse out assembly instructions ...")
print("*********************************************\n")

files = os.listdir("objdumpOutput")

# collection of all of the documents' instructions
test_corp_doc = open("test_corp_doc.txt", "w+")

# list_of_texts = []

for filename in files:
    if "corp" not in filename:
        o_path = "./objdumpOutput/" + filename
        i_path = "./parsedInstructions/" + filename[:-4] + "_INSTRUCTIONS.txt"

        o = open(o_path, 'r')
        i = open(i_path, 'w+')

        print("Parsing through sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        t, l = parseInstructions(o, i, i_path)
        # l = parseInstructions(o, i, i_path)

        # get a list of the lists of instructions to make a training set 
        # list_of_texts.append(t)

        test_corp_doc.write(l)
        test_corp_doc.write('\n')

        o.close()
        i.close()

# get_training_set(list_of_texts)
train_corp_doc = open("../training_samples/train_corp_doc.txt", "r")
train_corpus = list(read_corpus("../training_samples/train_corp_doc.txt"))

# print(train_corpus[:2])

test_corpus = list(read_corpus("test_corp_doc.txt", tokens_only=True))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

ranks = []
second_ranks = []
for doc_id in range(len(train_corpus)):
    inferred_vector = model.infer_vector(train_corpus[doc_id].words)
    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
    rank = [docid for docid, sim in sims].index(doc_id)
    ranks.append(rank)

    second_ranks.append(sims[1])

counter = collections.Counter(ranks)
print(counter)

print('Document ({}): «{}»\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
    print(u'%s %s: «%s»\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))

# Pick a random document from the corpus and infer a vector from the model
import random
doc_id = random.randint(0, len(train_corpus) - 1)

# Compare and print the second-most-similar document
print('Train Document ({}): «{}»\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
sim_id = second_ranks[doc_id]
print('Similar Document {}: «{}»\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))

# Pick a random document from the test corpus and infer a vector from the model
doc_id = random.randint(0, len(test_corpus) - 1)
inferred_vector = model.infer_vector(test_corpus[doc_id])
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

# Compare and print the most/median/least similar documents from the train corpus
# print('Test Document ({}): «{}»\n'.format(doc_id, ' '.join(test_corpus[doc_id])))
print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
    print(u'%s %s: «%s»\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))

train_corp_doc.close()
test_corp_doc.close()