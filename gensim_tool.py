import os
import sys
import gensim
import pprint
from gensim import corpora
from collections import defaultdict
import collections
import hashlib

################################ BEGIN FUNCTION DEFINITIONS ################################

#d - opens all the files in the specified directory for reading
#p - opens all the files in parseInstructions directory for writing
#piPath - creates new files in parseInstructions directory; appends _INSTRUCTIONS.txt
def parseInstructions(d, p, piPath):
    lines = d.readlines() 

    for line in lines:
        if ("..." in line):
            continue

        if(len(line.split('\t')) >= 3):
            command = line.split('\t')[2]
            instruction = command.split(" ")[0]
        else:
            continue

        p.write(instruction + '\n')

    p.close()

    # same file you just closed but opened read-only
    instructions = open(piPath, "r")
    p = instructions.read()

    # texts is a list of all of the instructions
    texts = [word for word in p.lower().split("\n")]
    
    # remove empty items from texts
    for text in texts:
        if (text == " ") or (text == ""):
            texts.remove(text)

    line = ""
    
    for text in texts:
        line = line + " " + text

    line = ''.join([w for w in line if not w.isdigit()])
    
    return texts, line[1:]

def read_corpus(fname, tokens_only=False):
    with open(fname, encoding="iso-8859-1") as f:
        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            if tokens_only:
                yield tokens
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

<<<<<<< HEAD
=======
# def languageAnalysis(corp):
#     for line in enumerate(corp):
#         tokens = gensim.utils.simple_preprocess(line)
#         yield tokens
>>>>>>> 1182ba6c1ac35bbbe8ab4ea957f8ceee458c0cf3

################################# END FUNCTION DEFINITIONS #################################

# change directory to where the all samples are stored
if len(sys.argv) == 2:
    directory = sys.argv[1]
elif len(sys.argv) == 1:
    print("Please add the dir where samples are stored as an argument")
elif len(sys.argv) > 2:
    print("Invalid # of arguemnts - too many")

# create dir that disassembled binaries will go into
# outputDir_path = directory + "/objdumpOutput"

# if not os.path.isdir(outputDir_path):
#      os.mkdir(outputDir_path)

# print("\n*********************************************")
# print("Preparing to disassemble malware samples ...")
# print("*********************************************\n")

os.chdir(directory)

files = os.listdir('.')
<<<<<<< HEAD
if "objdumpOutput" in files:
    files.remove('objdumpOutput')
if "parsedInstructions" in files:
    files.remove('parsedInstructions')
if "test_corp_doc.txt" in files:
    files.remove("test_corp_doc.txt")
if "train_corp_doc.txt" in files:
    files.remove("train_corp_doc.txt")
if "training_samples" in files:
    files.remove("training_samples")
=======
>>>>>>> 1182ba6c1ac35bbbe8ab4ea957f8ceee458c0cf3

for filename in files:
    if "corp" not in filename:
        objdumpOutput_path = "./objdumpOutput/" + filename + ".txt"
        print(objdumpOutput_path)
        print("Disassembling sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        cmd = "objdump -d " + filename + " > " + objdumpOutput_path
        os.system(cmd)

# for file in files:
#     objdumpOutput_path = "./objdumpOutput/" + file + ".txt"
#     print(objdumpOutput_path)
#     print("Disassembling sample " + str((files.index(file) + 1)) + " of " + str(len(files)) + "...")
#     cmd = "objdump -d " + file + " > " + objdumpOutput_path
#     os.system(cmd)

# print("\n! Completed disassmbling samples !\n")

#####################uncomment when ready to disassemble binaries###########################

# # REMINDER: still in the malware folder
instructions_path = directory + "parsedInstructions"

if not os.path.isdir("parsedInstructions"):
    os.mkdir("parsedInstructions")

print("\n*********************************************")
print("Preparing to parse out assembly instructions ...")
print("*********************************************\n")

# files = os.listdir("objdumpOutput")

# collection of all of the documents' instructions
test_corp_doc = open("test_corp_doc.txt", "w+")

test_corp_dict = {}

for filename in files:
    if "corp" not in filename:
        o_path = "./objdumpOutput/" + filename
        i_path = "./parsedInstructions/" + filename[:-4] + "_INSTRUCTIONS.txt"

        o = open(o_path, 'r')
        i = open(i_path, 'w+')

        print("Parsing through sample " + str((files.index(filename) + 1)) + " of " + str(len(files)) + "...")
        t, l = parseInstructions(o, i, i_path)

        test_corp_doc.write(l)
        test_corp_doc.write('\n')

        key = l[:50]
        test_corp_dict[key] = filename[:-4]

        o.close()
        i.close()

test_corp_doc.close()

pprint.pprint(test_corp_dict)

train_corp_doc = open("test_corp_doc.txt", "r")
train_corpus = list(read_corpus("test_corp_doc.txt"))

test_corpus = list(read_corpus("test_corp_doc.txt", tokens_only=True))


model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

# ranks = []
# second_ranks = []
# for doc_id in range(len(train_corpus)):
#     inferred_vector = model.infer_vector(train_corpus[doc_id].words)
#     sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
#     rank = [docid for docid, sim in sims].index(doc_id)
#     ranks.append(rank)
#     second_ranks.append(sims[1])


# Pick a random document from the test corpus and infer a vector from the model
for doc_id in range(len(test_corpus)):
    # doc_id = random.randint(0, len(test_corpus) - 1)
    inferred_vector = model.infer_vector(test_corpus[doc_id])
    # print(inferred_vector)
    sims_test = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))

    # Compare and print the most/median/least similar documents from the train corpus
    print('\n\nTest Document ({}): «{}»\n'.format(doc_id, (' '.join(test_corpus[doc_id])[:30]+"... ")))
    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)


    for label, index in [('MOST', 0), ('SECOND MOST', 1), ('MEDIAN', len(sims_test)//2), ('LEAST', len(sims_test) - 1)]:
        sample = ' '.join(train_corpus[sims_test[index][0]][0])
        sample_key = sample[:50]
        sample_name = test_corp_dict[sample_key]

        print(u'%s %s: «%s»\n' % (label, sims_test[index], sample_name))

train_corp_doc.close()
test_corp_doc.close()